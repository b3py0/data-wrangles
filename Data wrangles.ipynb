{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data wrangling and useful operations\n",
    "\n",
    "4th January 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inspect columns\n",
    "df.columns\n",
    "df.columns[0:9] #and every other splicing combination\n",
    "\n",
    "## Rename columns\n",
    "df['BASE'] = df['depth']\n",
    "\n",
    "##Rename selected columns from a dictionary\n",
    "df.rename(columns={'col1':'a',\n",
    "                   'col3':'c',\n",
    "                   'col25':'zz'}, inplace=True)\n",
    "\n",
    "## Check names of columns by slice\n",
    "list(df.columns.values[0:9])\n",
    "\n",
    "## Make a list of all the uniques values in a column:\n",
    "list(df['column name'].unique())\n",
    "\n",
    "## Get a value count of unique elements in a column\n",
    "info['SITE'].value_counts()\n",
    "\n",
    "##Sort by 2 columns\n",
    "df = df.sort_values(['SITE', 'BASE'], ascending=[True, True])\n",
    "\n",
    "## Sort a np object\n",
    "new_labels = sorted(Y_train.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add new columns in order with a function\n",
    "new_columns = ['AGE',\n",
    "               'STAGE',\n",
    "               'FORMATION',\n",
    "               'ZONE']\n",
    "\n",
    "def column_updates(new_columns,df):\n",
    "    df_columns=df.columns\n",
    "    new_columns=new_columns\n",
    "    \n",
    "    #Split off the original columns as a list\n",
    "    other_columns = list(df_columns.values) \n",
    "    \n",
    "    #Add the columns to the original df\n",
    "    new_df = pd.concat([df,pd.DataFrame(columns=list(new_columns))], sort=False)\n",
    "    \n",
    "    #order the columns\n",
    "    new_col_order = new_columns + other_columns\n",
    "    set(new_df.columns)==set(new_col_order)\n",
    "    x =new_df[new_col_order]\n",
    "    return x\n",
    "\n",
    "## Calling the function\n",
    "df = column_updates(new_columns, df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moving the last column to the front\n",
    "\n",
    "# 1) get a list of columns\n",
    "cols = list(df)\n",
    "\n",
    "# 2) move the column to head of list using index, pop and insert\n",
    "cols.insert(0, cols.pop(cols.index('col_name')))\n",
    "df = df.loc[:, cols]\n",
    "\n",
    "## Alternatively (and more intuitive)\n",
    "cols = df.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df = df[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inserting the same value to an empty column\n",
    "df.loc[df['SITE'] == 'F4','ANALYST'] = 'BioStrat' \n",
    "#This inserts 'Biostrat' into the column 'ANALYST' where the value 'F4' appears in the 'SITE' column\n",
    "\n",
    "## Updating a value based on column conditions and the index\n",
    "df.loc[(df['SITE'] == 'F1') & (df['BASE'].between(3270,3270)),'ZONE'] = 'Unassigned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Completing a column based on condition from a list\n",
    "my_list = ['W-TX', 'C-TX','HSTN', 'E-TX', 'LA', 'AL', 'MS']\n",
    "\n",
    "## Add a column condition based on a list \n",
    "df['NEW_COL'] = np.where(df.COLUMN_NAME.isin(my_list),'X','Y') #First val (x) is if true, 'y' if false\n",
    "\n",
    "##Replacing values within a column/array based on a dictionary\n",
    "replace_labels = {'AL':'Alabama',\n",
    "              'C-TX':'Central TX',\n",
    "              'E-TX':'East TX',\n",
    "              'HSTN':'HSTN embayment',\n",
    "              'LA':'Louisiana',\n",
    "              'MS':'Mississippi',\n",
    "              'W-TX':'West TX'}\n",
    "\n",
    "Y_train.replace(replace_labels, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop a column. Need to specify an axis. \n",
    "df.drop('Whatever', axis=1, inplace=True)\n",
    "\n",
    "##Drop a column when importing a df\n",
    "df = pd.read_csv('df_name.csv').drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add a column of row sums based on several columns:\n",
    "df_new['sum'] = df_new.iloc[:,15:].sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Combine two lists, drop duplicates and save as a csv\n",
    "\n",
    "## refactor columns into a list\n",
    "df1 = (list(df.columns)) #list 1\n",
    "df2 = (list(df0.columns)) #list 2\n",
    "\n",
    "# combine them\n",
    "combined = df1 + df0 \n",
    "combined = list(dict.fromkeys(combined)) #reshape into a dictionary to remove duplicate entries\n",
    "\n",
    "del combined[0:5] #Drop the first 5 items\n",
    "combined.sort() #sort items\n",
    "\n",
    "## save as a csv file\n",
    "with open('mylist.csv', \"w\") as outfile:\n",
    "    outfile.write(\"\\n\".join(str(item) for item in combined))\n",
    "\n",
    "\n",
    "## Less clunky: use SETS\n",
    "ton = list(toronja.columns)[5:] #drop first 5 columns\n",
    "can = list(canah.columns)[5:]\n",
    "\n",
    "combined_a = list(set(ton) | set(can))\n",
    "combined_a.sort()\n",
    "\n",
    "## Alternatively:\n",
    "combined_b = list(ton + can)\n",
    "combined_b.sort()\n",
    "combined_b = list(set(combined_b))\n",
    "\n",
    "'''\n",
    "set arguments (- in one but not both, | in one or 2 or both, & in one and 2, ^ in one or 2 but not both)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for suplicates entries in an index (e.g. timestamp information). Returns a list of values if duplicates:\n",
    "\n",
    "dups = dummy_df.index.duplicated().astype('str')\n",
    "\n",
    "d = []\n",
    "\n",
    "for dup in dups:\n",
    "    if str(True) in dup:\n",
    "        d.append(dup)\n",
    "d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subsetting information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split off columns by slice\n",
    "info = volve.iloc[:,0:17] #needs iloc\n",
    "\n",
    "## Split off by two conditions in 2 different columns\n",
    "def subset(name_item):\n",
    "    name_item = df.loc[(df['column1'] == name_item)] #name is the name of the item to class you are subsetting\n",
    "    name_item = name_item.sort_values(by=['column2'], ascending=False)\n",
    "    return name_item\n",
    "\n",
    "## call function\n",
    "f15a = subset('F15A')\n",
    "\n",
    "## Regular subset\n",
    "df.loc[(df['col_name'] == 'condition')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete rows by a condition/label\n",
    "df = df[~df['SITE'].isin(['BLACK'])] #drops the label 'BLACK' in the column called 'SITES'\n",
    "\n",
    "'''\n",
    "\"The bitwise operator ~ (pronounced as tilde) is a complement operator. \n",
    "It takes one bit operand and returns its complement. If the operand is 1, it returns 0, \n",
    "and if it is 0, it returns 1\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Delete rows with all zeros\n",
    "df_new = df.loc[(df!=0), any(1)]\n",
    "\n",
    "## Delete rows based on column condition:\n",
    "df_new.drop(df_new[df_new['sum'] == 0].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get number of unique observations of a string value in a df column:\n",
    "df.loc[(df.COL1 == 'OFFSHORE')].COL2.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Find all unique values within a dataframe\n",
    "list(pd.unique(df.values.ravel('K')))\n",
    "\n",
    "#In this case, using only some columns and all rows. Don't change K.\n",
    "list(pd.unique(offshore.iloc[:,16:].values.ravel('K'))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Groupby to get the difference between max and min values by class\n",
    "result = df.groupby(['col1','col2', 'col3'])['col4'].agg(['max','min'])\n",
    "result['col5'] = result['max']-result['min']\n",
    "print(result[['col5']]))\n",
    "\n",
    "\n",
    "## How many observations?\n",
    "df.groupby(['col1', 'col2']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Subsetting by one column, grouping values, counting, and sorting observations:\n",
    "\n",
    "df.loc[(df.col1 == 'Lutetian')].groupby(['col2', 'col3']).size().sort_values()\n",
    "\n",
    "## Or to order by one of the columns:\n",
    "df.loc[(df.col1 == 'Lutetian')].sort_values(['col2']).groupby(['col2','col3']).size()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch loading and concatenating csv\n",
    "\n",
    "This works in the same manner as merge in R. rows are matched by column and those that do not have NaN values. Pd.Merge does not operate in an intuitive way like merge in R."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modified from StackOverflow\n",
    "\n",
    "import glob \n",
    "import os\n",
    "path = r'C:\\Users\\....\\Input_files' # use appropriate path to files\n",
    "all_csv_files = glob.glob(os.path.join(path, \"*.csv\"))   #Specify .csv files\n",
    "\n",
    "df_from_each_file = (pd.read_csv(f, index_col=None) for f in all_files)\n",
    "concatenated_df   = pd.concat(df_from_each_file, ignore_index=True, sort=False) #sort=False turns off warnings.\n",
    "\n",
    "#If you wish to skiprows and drop rows etc:\n",
    "#This will skip the first row (that contains the header) and then drops the first row after removal of the header.\n",
    "df_from_each_file = (pd.read_csv(f, index_col=None, skiprows=1).drop([0], axis=0) for f in all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate dataframes\n",
    "perdido = pd.concat([df1, df2, df3],ignore_index=True, sort=False)\n",
    "\n",
    "# Sort df by site and by base (i.e. 2 different columns in order) so each site is in stratigraphic order\n",
    "perdido = perdido.sort_values(['SITE', 'BASE'], ascending=[True, True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save a list to .csv\n",
    "np.savetxt(\"Palaeocene_GoM_sites.txt\", x, delimiter=\",\",fmt='%s')\n",
    "\n",
    "#Save a groupby to .csv. Need to reset index\n",
    "y.reset_index().to_csv('week_grouped.csv')\n",
    "\n",
    "#Save only specific columns pf the groupby\n",
    "y.reset_index()[['col1','col2']].to_csv('week_grouped.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a prefix to all rows in a df\n",
    "df['COMMENT_new'] = 'FDO ' + df['Comment'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Deleting trailing and leading white space in a column (but preserving space in between words):\n",
    "## Turn into a list and strip white space\n",
    "prop_names = list(df.col1)\n",
    "\n",
    "## Remove trailing and leading white spaces:\n",
    "\n",
    "def normalize_space(s):\n",
    "    \"\"\"Return s stripped of leading/trailing whitespace\n",
    "    and with internal runs of whitespace replaced by a single SPACE\"\"\"\n",
    "    return ' '.join(s.split())\n",
    "\n",
    "replacement = [normalize_space(i) for i in prop_names] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import USA holidays\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "cal = calendar()\n",
    "holidays = cal.holidays(start='2014-12-30', end='2018-06-07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split dates into just days so that the holidays can map onto all the relevant hours of the day rather than just 00:00:00 in the first hour of a given holiday.\n",
    "day = dummy_df.index.map(lambda x: x.strftime('%Y-%m-%d')) #Note - this is a string and needs converting to dt object.\n",
    "\n",
    "dummy_df['Day'] = pd.to_datetime(day) #this is a dt object and in a seperate column\n",
    "dummy_df['Holidays'] =np.where(dummy_df['Day'].isin(holidays), 1,0) #first val if true =1, if false = 0\n",
    "dummy_df.drop('Day', axis=1, inplace=True) #get rid of excess column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Convert to df to hours from 15 minute divisions. \n",
    "pd.Grouper only works if you convert to datetime beforehand \n",
    "'''\n",
    "\n",
    "sales_hr = sales.groupby(pd.Grouper(freq='H')).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate new dates and place into the index\n",
    "holiday_dates = pd.date_range(start='2018-05-06 20:00:00', end='2018-06-06 20:00:00', freq='H') # these are hourly\n",
    "\n",
    "##Make a dummy dataframe for the holiday column if you want to add these into the future as well\n",
    "hol_df = pd.DataFrame(index=holiday_dates, columns=['Holidays'])\n",
    "hol_df.index.name = 'Date'\n",
    "hol_df['Holidays'] = 0 #Leaving as NaN will not allow for concat or merge so fill with a placeholder val.\n",
    "\n",
    "\n",
    "#Get a new index \n",
    "new_ix = df.index.append(holiday_dates)\n",
    "len(new_ix)\n",
    "\n",
    "#Concatenate with the original dataframe\n",
    "dummy_df = pd.concat([df, hol_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the days of the week and filter saturday and sunday\n",
    "dummy_df['Weekday'] = dummy_df.index.weekday \n",
    "\n",
    "# Monday is 0, Sunday is 6. Make a placeholder list to serahc over for weekends.\n",
    "\n",
    "wk = [5,6] #Saturday and Sunday\n",
    "dummy_df['Weekends'] =np.where(dummy_df['Weekday'].isin(wk), 1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hellinger transformation\n",
    "\n",
    "def hellinger_transformation(id_rows, df):\n",
    "    ''' id_rows --> this is the first column or columns and is a separate df.\n",
    "        df --> these columns are the counts (with the column headers) of the data matrix.'''\n",
    "    margin_total = df.sum(axis=1)\n",
    "    transform = df.apply(lambda x: np.sqrt(x / margin_total))\n",
    "    new_df = pd.concat([id_rows, transform], axis=1) \n",
    "    return new_df"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
